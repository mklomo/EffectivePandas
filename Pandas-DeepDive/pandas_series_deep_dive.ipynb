{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series Deep Dive\n",
    "\n",
    "This notebook covers core pandas series concepts using data from the US Fuel Economy Website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INNO\\AppData\\Local\\Temp\\ipykernel_52636\\390522536.py:5: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(url)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"./data/vehicles.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Assign the column 'city08' to a new variable\n",
    "city_mpg = df.city08\n",
    "\n",
    "# Assign the column 'highway08' to a new variable\n",
    "highway_mpg = df.highway08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19\n",
       "1         9\n",
       "2        23\n",
       "3        10\n",
       "4        17\n",
       "         ..\n",
       "41139    19\n",
       "41140    20\n",
       "41141    18\n",
       "41142    18\n",
       "41143    16\n",
       "Name: city08, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the city_mpg series \n",
    "city_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        25\n",
       "1        14\n",
       "2        33\n",
       "3        12\n",
       "4        23\n",
       "         ..\n",
       "41139    26\n",
       "41140    28\n",
       "41141    24\n",
       "41142    24\n",
       "41143    21\n",
       "Name: highway08, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the highway data\n",
    "highway_mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series Attributes\n",
    "\n",
    "Pandas provides several methods for working with Series. Built-in 'dir' function provides a list of all attributes available.\n",
    "- Dunder methods (.__add__, .__iter__, etc) provide many numeric operations, looping,\n",
    "attribute access, and index access. For the numeric operations, these return Series\n",
    "- Corresponding operator methods for many of the numeric operations allow us to tweak the\n",
    "behavior (there is an .add method in addition to .__add__).\n",
    "- Aggregate methods and properties which reduce or aggregate the values in a series down to\n",
    "a single scalar value. The .mean, .max, and .sum methods and .is_monotonic property are all\n",
    "examples.\n",
    "- Conversion methods. Some of these start with .to_ and export the data to other formats.\n",
    "- Manipulation methods such as .sort_values, .drop_duplicates, that return Series objects with\n",
    "the same index.\n",
    "- Indexing and accessor methods and attributes such as .loc and .iloc. These return Series or\n",
    "scalars.\n",
    "- String manipulation methods using .str.\n",
    "- Date manipulation methods using .dt.\n",
    "- Plotting methods using .plot.\n",
    "- Categorical manipulation methods using .cat.\n",
    "- Transformation methods such as .unstack and .reset_index, .agg, .transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dir(city_mpg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Operations\n",
    "\n",
    "We can apply most math operations on a series with another series and also use a scalar. When you operate with 2 series, pandas will align and index before performing the operation. Aligning will take each index entry in the left and match it up with every entry with the same name in the index of the right series. Values of the same index are added together.\n",
    "\n",
    "Hence, before performing vector operations, please make sure:\n",
    "- Indexes are unique\n",
    "- Indexes are common to both series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        22.0\n",
       "1        11.5\n",
       "2        28.0\n",
       "3        11.0\n",
       "4        20.0\n",
       "         ... \n",
       "41139    22.5\n",
       "41140    24.0\n",
       "41141    21.0\n",
       "41142    21.0\n",
       "41143    18.5\n",
       "Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(city_mpg + highway_mpg) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If these situations do not exist you will get missing values or a combinatoric explosion of results. Here is a simple example of two series that have repeated index entries as well as non-common entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      NaN\n",
       "2    120.0\n",
       "2    220.0\n",
       "2    130.0\n",
       "2    230.0\n",
       "4      NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series([10, 20, 30], index=[1, 2, 2])\n",
    "\n",
    "s2 = pd.Series([100, 200, 300], index=[2, 2, 4])\n",
    "\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "When performing math operations with a scalar, pandas broadcasts the operation to all values. There is another advantage to broadcasting. With many math operations, these are optimized and happen very quickly in the CPU. This is called vectorization. (A numeric pandas series is a block of memory, and modern CPUs leverage a technology called Single Instruction/Multiple Data (SIMD) to apply a math operation to the block of memory.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "> Note that there is also a .__iter__ method on a series, and you can loop over the items in a series. However, I recommend avoiding using a for loop with a series. That is a code smell, indicating that you are probably doing things the wrong way. You are removing one of the benefits of pandasâ€”vectorization and operating at the C level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "Chaining manipulations make life easier. Chaining makes the code easy to read and understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        22.0\n",
       "1        11.5\n",
       "2        28.0\n",
       "3        11.0\n",
       "4        20.0\n",
       "         ... \n",
       "41139    22.5\n",
       "41140    24.0\n",
       "41141    21.0\n",
       "41142    21.0\n",
       "41143    18.5\n",
       "Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    city_mpg.add(highway_mpg)\n",
    "    .div(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Methods\n",
    "\n",
    "These collapse the values of a series down to a scalar. Aggregations are the numbers that we report.\n",
    "For e.g. if your superior came in and asked for a sales report, the reply will be:\n",
    "- How many people came in (count)\n",
    "- How much food was ordered (count)\n",
    "- What was the total revenue (sum)\n",
    "- When did people come in (skew)\n",
    "\n",
    "Aggregations allow you to take a detailed data sample and collapse it to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.369045304297103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the mean \n",
    "city_mpg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregation Properties\n",
    "city_mpg.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Quantile Method\n",
    "city_mpg.quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25    15.0\n",
       "0.50    17.0\n",
       "0.75    20.0\n",
       "Name: city08, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To return multiple Quantlie values\n",
    "city_mpg.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and Mean of an Attribute\n",
    "\n",
    "To count values that meet some criteria, the sum method can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10272"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(city_mpg\n",
    " .gt(20) # Returns a boolean series\n",
    " .sum() # Returns the sum of the boolean series\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.965973167412017"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(city_mpg\n",
    " .gt(20) # Returns a boolean series \n",
    " .mul(100) # Returns a series of the same length as the boolean series after multiplying by 100\n",
    " .mean() # Returns the mean of the series\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The code above works because Python treats True as 1 and False as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .agg and Aggregation Strings\n",
    "\n",
    "The .agg method takes aggregations a step further but can transform the data in other ways depending on how it is called.\n",
    "Where .agg shines is in the ability to perform multiple aggregations. In that case, it returns a series. You can pass in the names of aggregations methods, NumPy reduction functions, Python aggregations, or define your own aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean           18.369045\n",
       "var            62.503036\n",
       "last_value     16.000000\n",
       "max           150.000000\n",
       "quantile       17.000000\n",
       "Name: city08, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining my own function\n",
    "def last_value(s):\n",
    "    \"\"\"Returns the last value in a series\"\"\"\n",
    "    return s.iloc[-1]\n",
    "(city_mpg\n",
    " .agg([\"mean\",np.var, last_value, \"max\", \"quantile\"]) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion Methods\n",
    "\n",
    "Sometimes there is the need to change the type of data being used. This may be due to formats that do not include type information, or the need to utilize more memory.\n",
    "\n",
    "The city_mpg data is loaded with dtype = int64. This is an overkill as the the data ranges between `6 - 150`. To specify the type of  for a series, the `.astype()` method can be used. Using the correct type can save significant amounts of memory. The default numeric type is 8 bytes wide (64 bits, ie int64 or float64). If you can use a narrower type, you can cut back on memory usage, giving you memory to process more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    41144.000000\n",
       "mean        18.369045\n",
       "std          7.905886\n",
       "min          6.000000\n",
       "25%         15.000000\n",
       "50%         17.000000\n",
       "75%         20.000000\n",
       "max        150.000000\n",
       "Name: city08, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-128, max=127, dtype=int8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting limits of data types\n",
    "import numpy as np\n",
    "\n",
    "# Returns the information about the data type uint8\n",
    "# np.iinfo(\"uint8\")\n",
    "\n",
    "# Returns the information about the data type np.int8\n",
    "np.iinfo(np.int8)\n",
    "\n",
    "# Returns the information about the data type np.int16\n",
    "# np.iinfo(np.int16)\n",
    "\n",
    "# Returns the information about the data type np.int32\n",
    "# np.iinfo(np.int32)\n",
    "\n",
    "# Returns the information about the data type np.int64\n",
    "# np.iinfo(np.int64)\n",
    "\n",
    "# Returns information about the data type float 16\n",
    "# np.finfo(np.float16)\n",
    "\n",
    "# Returns information about the data type float 16\n",
    "# np.finfo(np.float32)\n",
    "\n",
    "# Returns information about the data type float 16\n",
    "# np.finfo(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    41144.000000\n",
       "mean        18.369045\n",
       "std          7.905886\n",
       "min          6.000000\n",
       "25%         15.000000\n",
       "50%         17.000000\n",
       "75%         20.000000\n",
       "max        150.000000\n",
       "Name: city08, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying a type\n",
    "\n",
    "(city_mpg\n",
    " .astype(np.int16) \n",
    ").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After the dtype conversion, please cross-check the min and max values to ensure that they are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage\n",
    "\n",
    "To determine memory usage of the series, `nbytes` property or the `.memory_usage` methods can be used. For strings/object dtypes, use the `.memory_usage(deep=True)` to include\n",
    "the amount of memory used by the Python objects in the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329152"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    city_mpg\n",
    "    .nbytes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    city_mpg\n",
    "    # Convert to int16\n",
    "    .astype(np.int16)\n",
    "    # Check the memory usage\n",
    "    .nbytes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using .nbytes with object types only shows how much memory the Pandas object is taking.\n",
    "The make of the autos has strings and is stored as an object. To get the amount of memory that\n",
    "includes the strings, we need to use the .memory_usage method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2606395"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(\n",
    "    df['make']\n",
    "    .memory_usage(deep=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The value of .nbytes is just the memory that the data is using and not the ancillary parts of the\n",
    "Series. The .memory_usage includes the index memory and can include the contribution from object\n",
    "types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chevrolet                      4003\n",
       "Ford                           3371\n",
       "Dodge                          2583\n",
       "GMC                            2494\n",
       "Toyota                         2071\n",
       "                               ... \n",
       "Volga Associated Automobile       1\n",
       "Panos                             1\n",
       "Mahindra                          1\n",
       "Excalibur Autos                   1\n",
       "London Coach Co Inc               1\n",
       "Name: make, Length: 136, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How does the make data look like?\n",
    "\n",
    "(\n",
    "    df['make']\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to categorical Variable\n",
    "\n",
    "Categorical strings are useful for string data and can result in large memory savings. When strings are converted into categorical data, pandas no longer uses Python strings for each value but optimizes it, so repeating values are not duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95888"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets Convert The make data to category a categorical variable\n",
    "\n",
    "(\n",
    "    df['make']\n",
    "    # Convert to category\n",
    "    .astype('category')\n",
    "    # Check the memory usage\n",
    "    .memory_usage(deep=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Categories\n",
    "\n",
    "To create ordered categories, you need to define your own CategoricalDtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19\n",
       "1         9\n",
       "2        23\n",
       "3        10\n",
       "4        17\n",
       "         ..\n",
       "41139    19\n",
       "41140    20\n",
       "41141    18\n",
       "41142    18\n",
       "41143    16\n",
       "Name: city08, Length: 41144, dtype: category\n",
       "Categories (105, int64): [6 < 7 < 8 < 9 ... 137 < 138 < 140 < 150]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sorted list of the of the unique values\n",
    "values = pd.Series(sorted(set(city_mpg)))\n",
    "\n",
    "# Set the values as an ordered categorical data type \n",
    "city_type = pd.CategoricalDtype(categories=values, ordered=True)\n",
    "\n",
    "# Set the city_mpg column to the city_type data type\n",
    "city_mpg.astype(city_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       city08\n",
       "0          19\n",
       "1           9\n",
       "2          23\n",
       "3          10\n",
       "4          17\n",
       "...       ...\n",
       "41139      19\n",
       "41140      20\n",
       "41141      18\n",
       "41142      18\n",
       "41143      16\n",
       "\n",
       "[41144 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Series to a DataFrame\n",
    "\n",
    "city_mpg.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation Methods\n",
    "\n",
    "The manipulation methods are the workhorses of pandas. They can be used to understand, clean data.\n",
    "\n",
    "### .apply and .where\n",
    "The `.apply` method should be avoided whenever possible. It applies a function element-wise to every value. Numpy function broadcasts the operation to the series but the `.apply` method typically operates on each individual value in a series. If you have one million values in a series, it will be called one million times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Functions\n",
    "import timeit\n",
    "\n",
    "def gt20(val):\n",
    "    \"\"\"Check return if the value is greater than 20\"\"\"\n",
    "    return val > 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The code below the broadcasted and hence 50 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.96 ms Â± 1.46 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(\n",
    "    city_mpg\n",
    "    .apply(gt20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the code below, I want to limit the make column in the dataset to the top 5 makes and everything else `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the make of a car\n",
    "make = df['make']\n",
    "\n",
    "top_5_makes = (\n",
    "                make\n",
    "                # Fet the value counts for each make\n",
    "                .value_counts()\n",
    "                # Select the top 5 makes\n",
    "                .head(5)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79 ms Â± 57.9 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Keep the top 5 makes and replace the rest with Other\n",
    "\n",
    "(\n",
    "                make\n",
    "                # Where the make is in the top 5 makes keep it else fill with \"Other\"\n",
    "                .where(make.isin(top_5_makes), other='Other')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could define a function that does same\n",
    "def generalize_top5(val):\n",
    "    \"\"\"Generalize the top 5 makes\"\"\"\n",
    "    if val in top_5_makes:\n",
    "        return val\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.9 ms Â± 685 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "make.apply(generalize_top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using the where methods works faster than the apply method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Else with Pandas\n",
    "\n",
    "If I wanted to keep the top five makes and use Top10 for the remainder of the top ten makes, with\n",
    "Other for the rest, there is no built-in pandas method to do that. I could use the following function in combination with .apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Other\n",
       "1        Other\n",
       "2        Dodge\n",
       "3        Dodge\n",
       "4        Other\n",
       "         ...  \n",
       "41139    Other\n",
       "41140    Other\n",
       "41141    Other\n",
       "41142    Other\n",
       "41143    Other\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = make.value_counts()\n",
    "\n",
    "top_5 = vc.index[:5]\n",
    "\n",
    "top_10 = vc.index[:10]\n",
    "\n",
    "def generalize(val):\n",
    "    \"\"\"Generalize the top 5 makes\"\"\"\n",
    "    if val in top_5:\n",
    "        return val\n",
    "    elif val in top_10:\n",
    "        return 'Top10'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "make.apply(generalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Other\n",
       "1        Other\n",
       "2        Dodge\n",
       "3        Dodge\n",
       "4        Other\n",
       "         ...  \n",
       "41139    Other\n",
       "41140    Other\n",
       "41141    Other\n",
       "41142    Other\n",
       "41143    Other\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To replicate this in pandas\n",
    "\n",
    "(\n",
    "    make\n",
    "    # Where the make is in the top 5 makes keep it else fill with \"Other\"\n",
    "    .where(make.isin(top_5), 'Top10')\n",
    "    .where(make.isin(top_10), 'Other')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is using the select function from the NumPy Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other', 'Other', 'Dodge', ..., 'Other', 'Other', 'Other'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.select([make.isin(top_5), make.isin(top_10)], [make, 'Top10'], 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above code returns a NumPy array which can then be converted to a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Other\n",
       "1        Other\n",
       "2        Dodge\n",
       "3        Dodge\n",
       "4        Other\n",
       "         ...  \n",
       "41139    Other\n",
       "41140    Other\n",
       "41141    Other\n",
       "41142    Other\n",
       "41143    Other\n",
       "Length: 41144, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.select([make.isin(top_5), make.isin(top_10)], [make, 'Top10'], 'Other'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Filling in Missing Data is important because many machine learning models do not work if there is missing data. Also, it is prudent to be aware of how much data is missing to make sure you are getting the full story from your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyl_df = df[\"cylinders\"]\n",
    "\n",
    "(\n",
    "    cyl_df\n",
    "    # Check if data is missing\n",
    "    .isna()\n",
    "    # Sum the number of missing records with missing values\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7138     Nissan\n",
       "7139     Toyota\n",
       "8143     Toyota\n",
       "8144       Ford\n",
       "8146       Ford\n",
       "          ...  \n",
       "34563     Tesla\n",
       "34564     Tesla\n",
       "34565     Tesla\n",
       "34566     Tesla\n",
       "34567     Tesla\n",
       "Name: make, Length: 206, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get further context about which make models have missing data\n",
    "\n",
    "(\n",
    "    make\n",
    "    # Select make records with missing cylinder data\n",
    "    .loc[cyl_df.isna()]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Data\n",
    "\n",
    "It looks like Cylinder information is missing for electric vehicles. The `.fillna` method allows you to specify a replacement value for any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4.0\n",
       "1        12.0\n",
       "2         4.0\n",
       "3         8.0\n",
       "4         4.0\n",
       "         ... \n",
       "41139     4.0\n",
       "41140     4.0\n",
       "41141     4.0\n",
       "41142     4.0\n",
       "41143     4.0\n",
       "Name: cylinders, Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select record with missing cylinder value and fill with 0\n",
    "\n",
    "(\n",
    "    cyl_df\n",
    "    # Fill the missing cylinder data with 0\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the above operation returns a new series with the missing values replaced by zero. If I want to\n",
    "update my cyl variable, I would need to assign it to this new result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolating Data\n",
    "\n",
    "The `.interpolate()` method comes in handy if the data is ordered (as time series data often is) and there are holes in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32.0\n",
       "1    40.0\n",
       "2     NaN\n",
       "3    42.0\n",
       "4    39.0\n",
       "5    32.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interpolating ordered series\n",
    "\n",
    "temp = pd.Series([32, 40, None, 42, 39, 32])\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32.0\n",
       "1    40.0\n",
       "2    41.0\n",
       "3    42.0\n",
       "4    39.0\n",
       "5    32.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using interpolate to fill the data\n",
    "\n",
    "temp.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that index label 2 was missing, however, there are values for 1 and 3. After interpolation, the missing value becomes 41.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipping Data\n",
    "\n",
    "If you have outliers in your data, you might want to use the `.clip` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      19\n",
       "1       9\n",
       "2      23\n",
       "3      10\n",
       "4      17\n",
       "       ..\n",
       "442    15\n",
       "443    15\n",
       "444    15\n",
       "445    15\n",
       "446    31\n",
       "Name: city08, Length: 447, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg.loc[:446]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      19.0\n",
       "1      11.0\n",
       "2      20.0\n",
       "3      11.0\n",
       "4      17.0\n",
       "       ... \n",
       "442    15.0\n",
       "443    15.0\n",
       "444    15.0\n",
       "445    15.0\n",
       "446    20.0\n",
       "Name: city08, Length: 447, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    city_mpg\n",
    "    .loc[:446]\n",
    "    # Set values below the lower threshold to lower and values above the upper threshold to upper\n",
    "    .clip(\n",
    "        lower=city_mpg.quantile(0.05),\n",
    "        upper=city_mpg.quantile(0.75)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Values\n",
    "\n",
    "The `.sort_values` method will sort values in ascending order and also rearrange the index accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7901       6\n",
       "34557      6\n",
       "37161      6\n",
       "21060      6\n",
       "35887      6\n",
       "        ... \n",
       "34563    138\n",
       "34564    140\n",
       "32599    150\n",
       "31256    150\n",
       "33423    150\n",
       "Name: city08, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that because of index alignment, you can still do math operations (and many other\n",
    "operations) on a sorted series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        22.0\n",
       "1        11.5\n",
       "2        28.0\n",
       "3        11.0\n",
       "4        20.0\n",
       "         ... \n",
       "41139    22.5\n",
       "41140    24.0\n",
       "41141    21.0\n",
       "41142    21.0\n",
       "41143    18.5\n",
       "Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(city_mpg.sort_values() + highway_mpg) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Index\n",
    "\n",
    "The `.sort_index` method is used to sort the indexes of Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19\n",
       "1         9\n",
       "2        23\n",
       "3        10\n",
       "4        17\n",
       "         ..\n",
       "41139    19\n",
       "41140    20\n",
       "41141    18\n",
       "41142    18\n",
       "41143    16\n",
       "Name: city08, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg.sort_values().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Data\n",
    "\n",
    "The `.rank` method will return a series that keeps the original index but uses the ranks of values from the original series. By default, if two values are the same, their rank will be the average of the positions they take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        27060.5\n",
       "1          235.5\n",
       "2        35830.0\n",
       "3          607.5\n",
       "4        19484.0\n",
       "          ...   \n",
       "41139    27060.5\n",
       "41140    29719.5\n",
       "41141    23528.0\n",
       "41142    23528.0\n",
       "41143    15479.0\n",
       "Name: city08, Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        25555.0\n",
       "1          136.0\n",
       "2        35119.0\n",
       "3          336.0\n",
       "4        17467.0\n",
       "          ...   \n",
       "41139    25555.0\n",
       "41140    28567.0\n",
       "41141    21502.0\n",
       "41142    21502.0\n",
       "41143    13492.0\n",
       "Name: city08, Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To put equal values on the same rank,\n",
    "\n",
    "city_mpg.rank(method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        14.0\n",
       "1         4.0\n",
       "2        18.0\n",
       "3         5.0\n",
       "4        12.0\n",
       "         ... \n",
       "41139    14.0\n",
       "41140    15.0\n",
       "41141    13.0\n",
       "41142    13.0\n",
       "41143    11.0\n",
       "Name: city08, Length: 41144, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To not skip an positions\n",
    "\n",
    "city_mpg.rank(method='dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Data\n",
    "\n",
    "The `.replace` method allows mapping values to new values. You can specify a whole string to replace a string or use a dictionary to\n",
    "map old values to new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Alfa Romeo\n",
       "1           Ferrari\n",
       "2             Dodge\n",
       "3             Dodge\n",
       "4        Subaru-Not\n",
       "            ...    \n",
       "41139    Subaru-Not\n",
       "41140    Subaru-Not\n",
       "41141    Subaru-Not\n",
       "41142    Subaru-Not\n",
       "41143    Subaru-Not\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    make\n",
    "    # Replace 'Subaru' with 'Subaru-Not'\n",
    "    .replace('Subaru', 'Subaru-Not')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The to_replace parameterâ€™s value can contain a regular expression if you provide the regex=True\n",
    "parameter. In this example we use regular expression capture groups (they are specified in the\n",
    "expression by the parentheses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Alfa Romeo\n",
       "1            Ferrari\n",
       "2              Dodge\n",
       "3              Dodge\n",
       "4        ru-other-Su\n",
       "            ...     \n",
       "41139    ru-other-Su\n",
       "41140    ru-other-Su\n",
       "41141    ru-other-Su\n",
       "41142    ru-other-Su\n",
       "41143    ru-other-Su\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    make\n",
    "    # Using regex to replace 'Subaru'\n",
    "    .replace(regex=True, to_replace=r'(Su)ba(r.*)', value=r'\\2-other-\\1')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Data\n",
    "\n",
    "The `cut` method can be used to create equal bins of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         (15.6, 25.2]\n",
       "1        (5.856, 15.6]\n",
       "2         (15.6, 25.2]\n",
       "3        (5.856, 15.6]\n",
       "4         (15.6, 25.2]\n",
       "             ...      \n",
       "41139     (15.6, 25.2]\n",
       "41140     (15.6, 25.2]\n",
       "41141     (15.6, 25.2]\n",
       "41142     (15.6, 25.2]\n",
       "41143     (15.6, 25.2]\n",
       "Name: city08, Length: 41144, dtype: category\n",
       "Categories (15, interval[float64, right]): [(5.856, 15.6] < (15.6, 25.2] < (25.2, 34.8] < (34.8, 44.4] ... (111.6, 121.2] < (121.2, 130.8] < (130.8, 140.4] < (140.4, 150.0]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(city_mpg, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        (10, 20]\n",
       "1         (0, 10]\n",
       "2        (20, 40]\n",
       "3         (0, 10]\n",
       "4        (10, 20]\n",
       "           ...   \n",
       "41139    (10, 20]\n",
       "41140    (10, 20]\n",
       "41141    (10, 20]\n",
       "41142    (10, 20]\n",
       "41143    (10, 20]\n",
       "Name: city08, Length: 41144, dtype: category\n",
       "Categories (5, interval[int64, right]): [(0, 10] < (10, 20] < (20, 40] < (40, 80] < (80, 160]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying bin sizes\n",
    "\n",
    "pd.cut(city_mpg, bins=[0,10,20,40,80,160])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Operations\n",
    "\n",
    "Many index operations work on the `index position` while other work on the `index label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming index\n",
    "\n",
    "# Convert the make values to a dictionary\n",
    "make_dict = make.to_dict()\n",
    "\n",
    "# Rename the city_mpg index with the make dictionary\n",
    "city_mpg_2 = city_mpg.rename(make_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alfa Romeo', 'Ferrari', 'Dodge', 'Dodge', 'Subaru', 'Subaru', 'Subaru',\n",
       "       'Toyota', 'Toyota', 'Toyota',\n",
       "       ...\n",
       "       'Saab', 'Saturn', 'Saturn', 'Saturn', 'Saturn', 'Subaru', 'Subaru',\n",
       "       'Subaru', 'Subaru', 'Subaru'],\n",
       "      dtype='object', length=41144)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view the index\n",
    "\n",
    "city_mpg_2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alfa Romeo    19\n",
       "Ferrari        9\n",
       "Dodge         23\n",
       "Dodge         10\n",
       "Subaru        17\n",
       "              ..\n",
       "Subaru        19\n",
       "Subaru        20\n",
       "Subaru        18\n",
       "Subaru        18\n",
       "Subaru        16\n",
       "Name: city08, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the Index\n",
    "\n",
    "To reset the index to monotonic increasing, and therefor unique integers starting at zero, use the `.reset_index` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19\n",
       "1         9\n",
       "2        23\n",
       "3        10\n",
       "4        17\n",
       "         ..\n",
       "41139    19\n",
       "41140    20\n",
       "41141    18\n",
       "41142    18\n",
       "41143    16\n",
       "Name: city08, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that you can sort the values and the index with .sort_values and .sort_index respectively.\n",
    "Because those keep the same index, but just rearrange the order, they do not impact operations that\n",
    "align on the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .loc Attribute\n",
    "\n",
    "The ideal way of pulling data out by using indexing operators is by using the `loc` or `iloc` attributes.\n",
    "\n",
    "The `.loc` attributes deals with index labels. It allows you to pull out pieces of the series. The following can be passed into an index operation on `.loc`:\n",
    "\n",
    "- A scalar value of one of the index labels\n",
    "- A list of index labels\n",
    "- A slice of labels (closed interval so it includes the stop value)\n",
    "- An index\n",
    "- A boolean array\n",
    "- A function that accepts a series\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subaru    17\n",
       "Subaru    21\n",
       "Subaru    22\n",
       "Subaru    19\n",
       "Subaru    20\n",
       "          ..\n",
       "Subaru    19\n",
       "Subaru    20\n",
       "Subaru    18\n",
       "Subaru    18\n",
       "Subaru    16\n",
       "Name: city08, Length: 885, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg_2.loc[['Subaru']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subaru    17\n",
       "Subaru    21\n",
       "Subaru    22\n",
       "Subaru    19\n",
       "Subaru    20\n",
       "          ..\n",
       "Ford      26\n",
       "Ford      19\n",
       "Ford      21\n",
       "Ford      18\n",
       "Ford      19\n",
       "Name: city08, Length: 4256, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some more indexing\n",
    "\n",
    "city_mpg_2.loc[['Subaru', 'Ford']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After multiple operations, an intermediate object you are operating on might have a\n",
    "completely different index than the original object. By using a function, you will have access to the intermediate series and be able to create a row filter based on it. For series objects, this might seem like overkill, but it comes in very handy with dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Orange    3.850\n",
       "Pear      5.225\n",
       "Banana    6.600\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = pd.Series([1.00, 2.25, 3.50, 4.75, 6.00],\n",
    "          index = [\"Gum\", \"Apple\", \"Orange\", \"Pear\", \"Banana\"]\n",
    "          )\n",
    "\n",
    "inflation = 1.10\n",
    "\n",
    "(\n",
    "    cost\n",
    "    # Multiply by inflation\n",
    "    .mul(1.10)\n",
    "    # select records with costs > 3.50 based on the intermediate variable after mul\n",
    "    .loc[lambda val: val > 3.5]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .iloc Attribute\n",
    "\n",
    "This attribute is analogous to .loc but\n",
    "with a few differences. When we slice off of this attribute, we pull out items by index position. The .iloc attribute supports indexing with the following:\n",
    "\n",
    "- A scalar index position\n",
    "- A list of index positions\n",
    "- A slice of positions (half-open interval so it does not include stop value)\n",
    "- A NumPy array (or Python list) of boolean values\n",
    "- A function that accepts a series and returns one of the above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling out the first value\n",
    "\n",
    "city_mpg_2.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling out the last\n",
    " \n",
    " \n",
    "city_mpg_2.iloc[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alfa Romeo    19\n",
       "Subaru        16\n",
       "Name: city08, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling out specific records\n",
    "\n",
    "city_mpg_2.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alfa Romeo    19\n",
       "Ferrari        9\n",
       "Name: city08, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing the data\n",
    "\n",
    "city_mpg_2.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saab      18\n",
       "Saturn    23\n",
       "Saturn    21\n",
       "Saturn    24\n",
       "Saturn    21\n",
       "Subaru    19\n",
       "Subaru    20\n",
       "Subaru    18\n",
       "Subaru    18\n",
       "Subaru    16\n",
       "Name: city08, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the last 10 numbers\n",
    "\n",
    "city_mpg_2.iloc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nissan     81\n",
       "Toyota     81\n",
       "Toyota     81\n",
       "Ford       74\n",
       "Nissan     84\n",
       "         ... \n",
       "Tesla     140\n",
       "Tesla     115\n",
       "Tesla     104\n",
       "Tesla      98\n",
       "Toyota     55\n",
       "Name: city08, Length: 236, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = city_mpg_2 > 50 \n",
    "\n",
    "\n",
    "city_mpg_2.iloc[mask.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heads and Tails\n",
    "\n",
    "The `.head` and `.tail` methods are useful for pulling out values at the start or end of the series,\n",
    "respectively. These methods are used to quickly inspect a chunk of the data. The following code\n",
    "inspects the three values at the start and end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alfa Romeo    19\n",
       "Ferrari        9\n",
       "Dodge         23\n",
       "Dodge         10\n",
       "Subaru        17\n",
       "Name: city08, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subaru    19\n",
       "Subaru    20\n",
       "Subaru    18\n",
       "Subaru    18\n",
       "Subaru    16\n",
       "Name: city08, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg_2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "While `.head` and `.tail` allow us to inspect the data, sampling the data can be a better\n",
    "choice. Often the first few entries of the data may be incomplete, test data, or not representative\n",
    "of all of the values. Sampling might be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Volvo         16\n",
       "Mitsubishi    19\n",
       "Buick         27\n",
       "Jeep          15\n",
       "Land Rover    13\n",
       "Saab          17\n",
       "Mercury       20\n",
       "Name: city08, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling 7 records\n",
    "\n",
    "city_mpg_2.sample(7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Index Values\n",
    "\n",
    "The `.filter` method will filter index labels by exact match, substring, or regular expression. These are controlled with the mutually exclusive items, like, and regex parameters, respectively.\n",
    "\n",
    "> Note that exact match (with items) fails with duplicate index labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_mpg_2.filter(items=['Ford', 'Chevy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ford    18\n",
       "Ford    16\n",
       "Ford    17\n",
       "Ford    17\n",
       "Ford    15\n",
       "        ..\n",
       "Ford    26\n",
       "Ford    19\n",
       "Ford    21\n",
       "Ford    18\n",
       "Ford    19\n",
       "Name: city08, Length: 3371, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_mpg_2.filter(like='Ford')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subaru    17\n",
       "Subaru    21\n",
       "Subaru    22\n",
       "Ford      18\n",
       "Ford      16\n",
       "          ..\n",
       "Subaru    19\n",
       "Subaru    20\n",
       "Subaru    18\n",
       "Subaru    18\n",
       "Subaru    16\n",
       "Name: city08, Length: 4256, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using regex for filtering\n",
    "\n",
    "city_mpg_2.filter(regex='(Ford)|(Subaru)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing\n",
    "\n",
    "The `.reindex` method allows you to pull out values by index label. It will conform the series or\n",
    "return a series with the order of the index labels provided. Unlike .loc and .filter, you can pass in labels that are not in the index, and it will not throw an error. Rather it will insert missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_1 = pd.Series([10,20,30], index = ['a','b','c'])\n",
    "\n",
    "s_2 = pd.Series([100,200,300], index = ['x','b','c'])\n",
    "\n",
    "# Addition will not work due to different index labels\n",
    "# s_1 + s_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    100\n",
       "b    200\n",
       "c    300\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a      NaN\n",
       "b    200.0\n",
       "c    300.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_2.reindex(s_1.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings and Objects\n",
    "\n",
    "Pandas has a `string` type that supports missing values that are not `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Alfa Romeo\n",
       "1           Ferrari\n",
       "2             Dodge\n",
       "3             Dodge\n",
       "4            Subaru\n",
       "            ...    \n",
       "41139        Subaru\n",
       "41140        Subaru\n",
       "41141        Subaru\n",
       "41142        Subaru\n",
       "41143        Subaru\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The make column has an object type by default\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Alfa Romeo\n",
       "1           Ferrari\n",
       "2             Dodge\n",
       "3             Dodge\n",
       "4            Subaru\n",
       "            ...    \n",
       "41139        Subaru\n",
       "41140        Subaru\n",
       "41141        Subaru\n",
       "41142        Subaru\n",
       "41143        Subaru\n",
       "Name: make, Length: 41144, dtype: string"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the object type to string type\n",
    "\n",
    "make.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The difference b/n the `string` type and strings stored in object (and category) type series is that the string methods return a nullable type when you use a `string` series. If the\n",
    "result of the string method is missing, pandas will use the newer types that have native pandas\n",
    "nullable types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Strings\n",
    "\n",
    "If you have low cardinality string columns, consider using a categorical type for them. You will\n",
    "have access to many of the same string manipulation methods (though some are not available in\n",
    "this case). The main advantage here is memory savings and performance improvements, as the\n",
    "operations need to be done only on the individual categories and not each value in the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Alfa Romeo\n",
       "1           Ferrari\n",
       "2             Dodge\n",
       "3             Dodge\n",
       "4            Subaru\n",
       "            ...    \n",
       "41139        Subaru\n",
       "41140        Subaru\n",
       "41141        Subaru\n",
       "41142        Subaru\n",
       "41143        Subaru\n",
       "Name: make, Length: 41144, dtype: category\n",
       "Categories (136, object): ['AM General', 'ASC Incorporated', 'Acura', 'Alfa Romeo', ..., 'Volvo', 'Wallace Environmental', 'Yugo', 'smart']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .str Accessor\n",
    "\n",
    "The object, 'string', and 'category' types have a .str accessor that provides string manipulation\n",
    "methods. Most of these methods are modeled after the Python string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        alfa romeo\n",
       "1           ferrari\n",
       "2             dodge\n",
       "3             dodge\n",
       "4            subaru\n",
       "            ...    \n",
       "41139        subaru\n",
       "41140        subaru\n",
       "41141        subaru\n",
       "41142        subaru\n",
       "41143        subaru\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Make string lowercase\n",
    "make.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -1\n",
       "1       -1\n",
       "2       -1\n",
       "3       -1\n",
       "4        0\n",
       "        ..\n",
       "41139    0\n",
       "41140    0\n",
       "41141    0\n",
       "41142    0\n",
       "41143    0\n",
       "Name: make, Length: 41144, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More string accessors\n",
    "\n",
    "make.str.find('Subaru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching\n",
    "\n",
    "Searching through strings with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-    1727\n",
       ".      46\n",
       ",       9\n",
       "Name: make, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    make\n",
    "    # find all of the non alphabetic characters nad return as a series\n",
    "    .str.extract(r'([^a-z A-Z])', expand=False)\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "When dealing with survey data, you may come across binned numeric values. The survey probably\n",
    "had a drop-down of different ranges. It might have said, what is your age? And have options for\n",
    "20-29, 30-39, 40-49, etc. Those survey results come in as strings because pandas cannot handle the\n",
    "dash. Hence we cannot perform math operations on the ages, like calculating the minimum or\n",
    "mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0-10\n",
       "1    10-20\n",
       "2    20-30\n",
       "3    30-40\n",
       "4    40-50\n",
       "5    50-60\n",
       "6    60-70\n",
       "dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = pd.Series(['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70'])\n",
    "\n",
    "\n",
    "age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10\n",
       "1    20\n",
       "2    30\n",
       "3    40\n",
       "4    50\n",
       "5    60\n",
       "6    70\n",
       "Name: 1, dtype: int32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(\n",
    "    age\n",
    "    # Split based on the '-' character\n",
    "    .str.split('-', expand=True)\n",
    "    # Take the Upperside\n",
    "    .iloc[:, 1]\n",
    "    .astype(int)\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5.0\n",
       "1    15.0\n",
       "2    25.0\n",
       "3    35.0\n",
       "4    45.0\n",
       "5    55.0\n",
       "6    65.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    age\n",
    "    .str.split('-', expand=True)\n",
    "    .astype(int)\n",
    "    # the average of the bin ranges\n",
    "    .mean(axis='columns')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `.mean` is applied across each row (manipulating across the row is accomplished with the axis='columns' parameter). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Text\n",
    "\n",
    "\n",
    "Both the series and the .str attribute have a .replace method, and these methods have overlapping\n",
    "functionality. If I want to replace single characters, I typically use .str.replace, but if I have\n",
    "complete replacements for many of the values I use .replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Alfao Romeo\n",
       "1           Ferraori\n",
       "2              Dodge\n",
       "3              Dodge\n",
       "4            Subaoru\n",
       "            ...     \n",
       "41139        Subaoru\n",
       "41140        Subaoru\n",
       "41141        Subaoru\n",
       "41142        Subaoru\n",
       "41143        Subaoru\n",
       "Name: make, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below replaces all \"a\" with \"ao\" in the entire \n",
    "make.str.replace('a', 'ao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2015 -03 -08 08:00:00+00:00 \n",
       "1     2015 -03 -08 08:30:00+00:00 \n",
       "2     2015 -03 -08 09:00:00+00:00 \n",
       "3     2015 -03 -08 09:30:00+00:00 \n",
       "4     2015 -11 -01 06:30:00+00:00 \n",
       "5     2015 -11 -01 07:00:00+00:00 \n",
       "6     2015 -11 -01 07:30:00+00:00 \n",
       "7     2015 -11 -01 08:00:00+00:00 \n",
       "8     2015 -11 -01 08:30:00+00:00 \n",
       "9     2015 -11 -01 08:00:00+00:00 \n",
       "10    2015 -11 -01 08:30:00+00:00 \n",
       "11    2015 -11 -01 09:00:00+00:00 \n",
       "12    2015 -11 -01 09:30:00+00:00 \n",
       "13    2015 -11 -01 10:00:00+00:00 \n",
       "dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = pd.Series ([ '2015 -03 -08 08:00:00+00:00 ' ,\n",
    "                    '2015 -03 -08 08:30:00+00:00 ' ,\n",
    "                    '2015 -03 -08 09:00:00+00:00 ' ,\n",
    "                    '2015 -03 -08 09:30:00+00:00 ' ,\n",
    "                    '2015 -11 -01 06:30:00+00:00 ' ,\n",
    "                    '2015 -11 -01 07:00:00+00:00 ' ,\n",
    "                    '2015 -11 -01 07:30:00+00:00 ' ,\n",
    "                    '2015 -11 -01 08:00:00+00:00 ' ,\n",
    "                    '2015 -11 -01 08:30:00+00:00 ' ,\n",
    "                    '2015 -11 -01 08:00:00+00:00 ' ,\n",
    "                    '2015 -11 -01 08:30:00+00:00 ' ,\n",
    "                    '2015 -11 -01 09:00:00+00:00 ' ,\n",
    "                    '2015 -11 -01 09:30:00+00:00 ' ,\n",
    "                    '2015 -11 -01 10:00:00+00:00 '])\n",
    "\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 08:00:00+00:00\n",
       "1    2015-03-08 08:30:00+00:00\n",
       "2    2015-03-08 09:00:00+00:00\n",
       "3    2015-03-08 09:30:00+00:00\n",
       "4    2015-11-01 06:30:00+00:00\n",
       "5    2015-11-01 07:00:00+00:00\n",
       "6    2015-11-01 07:30:00+00:00\n",
       "7    2015-11-01 08:00:00+00:00\n",
       "8    2015-11-01 08:30:00+00:00\n",
       "9    2015-11-01 08:00:00+00:00\n",
       "10   2015-11-01 08:30:00+00:00\n",
       "11   2015-11-01 09:00:00+00:00\n",
       "12   2015-11-01 09:30:00+00:00\n",
       "13   2015-11-01 10:00:00+00:00\n",
       "dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert from object dtype to datetime frim the utc timezone\n",
    "\n",
    "utc_s = pd.to_datetime(col, utc=True)\n",
    "\n",
    "utc_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the result of the dtype. It indicates that the dates are stored as UTC. Once you have\n",
    "converted a series into a datetime64[ns] object, you have the ability to leverage the .dt attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 16:00:00+08:00\n",
       "1    2015-03-08 16:30:00+08:00\n",
       "2    2015-03-08 17:00:00+08:00\n",
       "3    2015-03-08 17:30:00+08:00\n",
       "4    2015-11-01 14:30:00+08:00\n",
       "5    2015-11-01 15:00:00+08:00\n",
       "6    2015-11-01 15:30:00+08:00\n",
       "7    2015-11-01 16:00:00+08:00\n",
       "8    2015-11-01 16:30:00+08:00\n",
       "9    2015-11-01 16:00:00+08:00\n",
       "10   2015-11-01 16:30:00+08:00\n",
       "11   2015-11-01 17:00:00+08:00\n",
       "12   2015-11-01 17:30:00+08:00\n",
       "13   2015-11-01 18:00:00+08:00\n",
       "dtype: datetime64[ns, Asia/Kuala_Lumpur]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert from datetime to Kuala Lumpur timezone\n",
    "\n",
    "utc_s.dt.tz_convert('Asia/Kuala_Lumpur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 16:00:00+08:00\n",
       "1    2015-03-08 16:30:00+08:00\n",
       "2    2015-03-08 17:00:00+08:00\n",
       "3    2015-03-08 17:30:00+08:00\n",
       "4    2015-11-01 14:30:00+08:00\n",
       "5    2015-11-01 15:00:00+08:00\n",
       "6    2015-11-01 15:30:00+08:00\n",
       "7    2015-11-01 16:00:00+08:00\n",
       "8    2015-11-01 16:30:00+08:00\n",
       "9    2015-11-01 16:00:00+08:00\n",
       "10   2015-11-01 16:30:00+08:00\n",
       "11   2015-11-01 17:00:00+08:00\n",
       "12   2015-11-01 17:30:00+08:00\n",
       "13   2015-11-01 18:00:00+08:00\n",
       "dtype: datetime64[ns, Asia/Kuala_Lumpur]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data with Offset\n",
    "\n",
    "s = pd.Series ([ '2015 -03 -08 01:00:00 -07:00 ' ,\n",
    "                    '2015 -03 -08 01:30:00 -07:00 ' ,\n",
    "                    '2015 -03 -08 03:00:00 -06:00 ' ,\n",
    "                    '2015 -03 -08 03:30:00 -06:00 ' ,\n",
    "                    '2015 -11 -01 00:30:00 -06:00 ' ,\n",
    "                    '2015 -11 -01 01:00:00 -06:00 ' ,\n",
    "                    '2015 -11 -01 01:30:00 -06:00 ' ,\n",
    "                    '2015 -11 -01 01:00:00 -07:00 ' ,\n",
    "                    '2015 -11 -01 01:30:00 -07:00 ' ,\n",
    "                    '2015 -11 -01 01:00:00 -07:00 ' ,\n",
    "                    '2015 -11 -01 01:30:00 -07:00 ' ,\n",
    "                    '2015 -11 -01 02:00:00 -07:00 ' ,\n",
    "                    '2015 -11 -01 02:30:00 -07:00 ' ,\n",
    "                    '2015 -11 -01 03:00:00 -07:00 ']\n",
    "               )\n",
    "\n",
    "pd.to_datetime(s, utc=True).dt.tz_convert('Asia/Kuala_Lumpur')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Local Time Data\n",
    "\n",
    "If we want to load local date information, we need to have the date, the offset, and the timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.Series ([ '2015 -03 -08 01:00:00 ' ,\n",
    "                    '2015 -03 -08 01:30:00 ' ,\n",
    "                    '2015 -03 -08 02:00:00 ' ,\n",
    "                    '2015 -03 -08 02:30:00 ' ,\n",
    "                    '2015 -03 -08 03:00:00 ' ,\n",
    "                    '2015 -03 -08 02:00:00 ' ,\n",
    "                    '2015 -03 -08 02:30:00 ' ,\n",
    "                    '2015 -03 -08 03:00:00 ' ,\n",
    "                    '2015 -03 -08 03:30:00 ' ,\n",
    "                    '2015 -11 -01 00:30:00 ' ,\n",
    "                    '2015 -11 -01 01:00:00 ' ,\n",
    "                    '2015 -11 -01 01:30:00 ' ,\n",
    "                    '2015 -11 -01 02:00:00 ' ,\n",
    "                    '2015 -11 -01 02:30:00 ' ,\n",
    "                    '2015 -11 -01 01:00:00 ' ,\n",
    "                    '2015 -11 -01 01:30:00 ' ,\n",
    "                    '2015 -11 -01 02:00:00 ' ,\n",
    "                    '2015 -11 -01 02:30:00 ' ,\n",
    "                    '2015 -11 -01 03:00:00 '])\n",
    "\n",
    "\n",
    "offset = pd.Series ([-7, -7, -7, -7, -7, -6, -6,\n",
    "                    -6, -6, -6, -6, -6, -6, -6, -7, -7, -7, -7, -7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To apply the offset to the corresponding time, use the `.groupby` with `.transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 16:00:00+08:00\n",
       "1    2015-03-08 16:30:00+08:00\n",
       "2    2015-03-08 17:00:00+08:00\n",
       "3    2015-03-08 17:30:00+08:00\n",
       "4    2015-03-08 18:00:00+08:00\n",
       "5    2015-03-08 16:00:00+08:00\n",
       "6    2015-03-08 16:30:00+08:00\n",
       "7    2015-03-08 17:00:00+08:00\n",
       "8    2015-03-08 17:30:00+08:00\n",
       "9    2015-11-01 14:30:00+08:00\n",
       "10   2015-11-01 15:00:00+08:00\n",
       "11   2015-11-01 15:30:00+08:00\n",
       "12   2015-11-01 16:00:00+08:00\n",
       "13   2015-11-01 16:30:00+08:00\n",
       "14   2015-11-01 16:00:00+08:00\n",
       "15   2015-11-01 16:30:00+08:00\n",
       "16   2015-11-01 17:00:00+08:00\n",
       "17   2015-11-01 17:30:00+08:00\n",
       "18   2015-11-01 18:00:00+08:00\n",
       "dtype: datetime64[ns, Asia/Kuala_Lumpur]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = offset.replace ({-7:'-07:00', -6: '-06:00 '})\n",
    "\n",
    "(\n",
    "    pd.to_datetime(time)\n",
    "    .groupby(offset)\n",
    "    .transform(lambda s: s.dt.tz_localize(s.name)\n",
    "                          .dt.tz_convert('Asia/Kuala_Lumpur'))               \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to UTC\n",
    "\n",
    "If you have a series with local time information (stored as datetime64[ns] and not a string), you can use the .dt.tz_convert method to change it to UTC time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 01:00:00+00:00\n",
       "1    2015-03-08 01:30:00+00:00\n",
       "2    2015-03-08 02:00:00+00:00\n",
       "3    2015-03-08 02:30:00+00:00\n",
       "4    2015-03-08 03:00:00+00:00\n",
       "5    2015-03-08 02:00:00+00:00\n",
       "6    2015-03-08 02:30:00+00:00\n",
       "7    2015-03-08 03:00:00+00:00\n",
       "8    2015-03-08 03:30:00+00:00\n",
       "9    2015-11-01 00:30:00+00:00\n",
       "10   2015-11-01 01:00:00+00:00\n",
       "11   2015-11-01 01:30:00+00:00\n",
       "12   2015-11-01 02:00:00+00:00\n",
       "13   2015-11-01 02:30:00+00:00\n",
       "14   2015-11-01 01:00:00+00:00\n",
       "15   2015-11-01 01:30:00+00:00\n",
       "16   2015-11-01 02:00:00+00:00\n",
       "17   2015-11-01 02:30:00+00:00\n",
       "18   2015-11-01 03:00:00+00:00\n",
       "dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local = pd.to_datetime(time)\n",
    "\n",
    "\n",
    "local.dt.tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Epochs\n",
    "\n",
    "You can get the seconds past the UNIX epoch from a UTC or local time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1425776400\n",
       "1     1425778200\n",
       "2     1425780000\n",
       "3     1425781800\n",
       "4     1425783600\n",
       "5     1425780000\n",
       "6     1425781800\n",
       "7     1425783600\n",
       "8     1425785400\n",
       "9     1446337800\n",
       "10    1446339600\n",
       "11    1446341400\n",
       "12    1446343200\n",
       "13    1446345000\n",
       "14    1446339600\n",
       "15    1446341400\n",
       "16    1446343200\n",
       "17    1446345000\n",
       "18    1446346800\n",
       "dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secs = local.view('int64').floordiv(1e9).astype('int64')\n",
    "\n",
    "secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 01:00:00+00:00\n",
       "1    2015-03-08 01:30:00+00:00\n",
       "2    2015-03-08 02:00:00+00:00\n",
       "3    2015-03-08 02:30:00+00:00\n",
       "4    2015-03-08 03:00:00+00:00\n",
       "5    2015-03-08 02:00:00+00:00\n",
       "6    2015-03-08 02:30:00+00:00\n",
       "7    2015-03-08 03:00:00+00:00\n",
       "8    2015-03-08 03:30:00+00:00\n",
       "9    2015-11-01 00:30:00+00:00\n",
       "10   2015-11-01 01:00:00+00:00\n",
       "11   2015-11-01 01:30:00+00:00\n",
       "12   2015-11-01 02:00:00+00:00\n",
       "13   2015-11-01 02:30:00+00:00\n",
       "14   2015-11-01 01:00:00+00:00\n",
       "15   2015-11-01 01:30:00+00:00\n",
       "16   2015-11-01 02:00:00+00:00\n",
       "17   2015-11-01 02:30:00+00:00\n",
       "18   2015-11-01 03:00:00+00:00\n",
       "dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Epoch information\n",
    "\n",
    "(\n",
    "    pd.to_datetime(secs, unit='s')\n",
    "    .dt.tz_localize('UTC')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DAPR</th>\n",
       "      <th>DASF</th>\n",
       "      <th>MDPR</th>\n",
       "      <th>MDSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TOBS</th>\n",
       "      <th>WT01</th>\n",
       "      <th>WT03</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT06</th>\n",
       "      <th>WT11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00420072</td>\n",
       "      <td>ALTA, UT US</td>\n",
       "      <td>40.5905</td>\n",
       "      <td>-111.6369</td>\n",
       "      <td>2660.9</td>\n",
       "      <td>1980-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00420072</td>\n",
       "      <td>ALTA, UT US</td>\n",
       "      <td>40.5905</td>\n",
       "      <td>-111.6369</td>\n",
       "      <td>2660.9</td>\n",
       "      <td>1980-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00420072</td>\n",
       "      <td>ALTA, UT US</td>\n",
       "      <td>40.5905</td>\n",
       "      <td>-111.6369</td>\n",
       "      <td>2660.9</td>\n",
       "      <td>1980-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00420072</td>\n",
       "      <td>ALTA, UT US</td>\n",
       "      <td>40.5905</td>\n",
       "      <td>-111.6369</td>\n",
       "      <td>2660.9</td>\n",
       "      <td>1980-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00420072</td>\n",
       "      <td>ALTA, UT US</td>\n",
       "      <td>40.5905</td>\n",
       "      <td>-111.6369</td>\n",
       "      <td>2660.9</td>\n",
       "      <td>1980-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION         NAME  LATITUDE  LONGITUDE  ELEVATION        DATE  DAPR  \\\n",
       "0  USC00420072  ALTA, UT US   40.5905  -111.6369     2660.9  1980-01-01   NaN   \n",
       "1  USC00420072  ALTA, UT US   40.5905  -111.6369     2660.9  1980-01-02   NaN   \n",
       "2  USC00420072  ALTA, UT US   40.5905  -111.6369     2660.9  1980-01-03   NaN   \n",
       "3  USC00420072  ALTA, UT US   40.5905  -111.6369     2660.9  1980-01-04   NaN   \n",
       "4  USC00420072  ALTA, UT US   40.5905  -111.6369     2660.9  1980-01-05   NaN   \n",
       "\n",
       "   DASF  MDPR  MDSF  ...  SNWD  TMAX  TMIN  TOBS  WT01  WT03  WT04  WT05  \\\n",
       "0   NaN   NaN   NaN  ...  29.0  38.0  25.0  25.0   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN  ...  34.0  27.0  18.0  18.0   NaN   NaN   NaN   NaN   \n",
       "2   NaN   NaN   NaN  ...  30.0  27.0  12.0  18.0   NaN   NaN   NaN   NaN   \n",
       "3   NaN   NaN   NaN  ...  30.0  31.0  18.0  27.0   NaN   NaN   NaN   NaN   \n",
       "4   NaN   NaN   NaN  ...  30.0  34.0  26.0  34.0   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   WT06  WT11  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/alta-noaa-1980-2019.csv'\n",
    "\n",
    "\n",
    "time_df = pd.read_csv(url)\n",
    "\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1980-01-01\n",
       "1       1980-01-02\n",
       "2       1980-01-03\n",
       "3       1980-01-04\n",
       "4       1980-01-05\n",
       "           ...    \n",
       "14155   2019-09-03\n",
       "14156   2019-09-04\n",
       "14157   2019-09-05\n",
       "14158   2019-09-06\n",
       "14159   2019-09-07\n",
       "Name: DATE, Length: 14160, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to the datetime\n",
    "dates = pd.to_datetime(time_df['DATE'])\n",
    "\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Given that the dates series has the `dtype=datetime64[ns]`, I can now use the `.dt` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Tuesday\n",
       "1        Wednesday\n",
       "2         Thursday\n",
       "3           Friday\n",
       "4         Saturday\n",
       "           ...    \n",
       "14155      Tuesday\n",
       "14156    Wednesday\n",
       "14157     Thursday\n",
       "14158       Friday\n",
       "14159     Saturday\n",
       "Name: DATE, Length: 14160, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the weekdays\n",
    "\n",
    "dates.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "14155    False\n",
       "14156    False\n",
       "14157    False\n",
       "14158    False\n",
       "14159    False\n",
       "Name: DATE, Length: 14160, dtype: bool"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date Properties\n",
    "\n",
    "dates.dt.is_month_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        2\n",
       "2        3\n",
       "3        4\n",
       "4        5\n",
       "        ..\n",
       "14155    1\n",
       "14156    2\n",
       "14157    3\n",
       "14158    4\n",
       "14159    5\n",
       "Name: DATE, Length: 14160, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More date properties\n",
    "\n",
    "dates.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1980-01-01\n",
       "1        1980-01-02\n",
       "2        1980-01-03\n",
       "3        1980-01-04\n",
       "4        1980-01-05\n",
       "            ...    \n",
       "14155    2019-09-03\n",
       "14156    2019-09-04\n",
       "14157    2019-09-05\n",
       "14158    2019-09-06\n",
       "14159    2019-09-07\n",
       "Name: DATE, Length: 14160, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date formatting\n",
    "\n",
    "dates.dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates in the Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-01    2.0\n",
       "1980-01-02    3.0\n",
       "1980-01-03    1.0\n",
       "1980-01-04    0.0\n",
       "1980-01-05    0.0\n",
       "             ... \n",
       "2019-09-03    0.0\n",
       "2019-09-04    0.0\n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: SNOW, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow = (\n",
    "    time_df['SNOW']\n",
    "    # Set the DATE as the index\n",
    "    .rename(dates)\n",
    ")\n",
    "\n",
    "snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985-07-30   NaN\n",
       "1985-09-12   NaN\n",
       "1985-09-19   NaN\n",
       "1986-02-07   NaN\n",
       "1986-06-26   NaN\n",
       "              ..\n",
       "2017-04-26   NaN\n",
       "2017-09-20   NaN\n",
       "2017-10-02   NaN\n",
       "2017-12-23   NaN\n",
       "2018-12-03   NaN\n",
       "Name: SNOW, Length: 365, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Missing Data\n",
    "\n",
    "snow[snow.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985-09-01    0.0\n",
       "1985-09-02    0.0\n",
       "1985-09-03    0.0\n",
       "1985-09-04    0.0\n",
       "1985-09-05    0.0\n",
       "             ... \n",
       "1985-10-27    0.0\n",
       "1985-10-28    0.0\n",
       "1985-10-29    0.0\n",
       "1985-10-30    0.0\n",
       "1985-10-31    5.0\n",
       "Name: SNOW, Length: 61, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date indexing from Sept - Oct 1985\n",
    "\n",
    "snow.loc['1985-09':'1985-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985-08-01    0.0\n",
       "1985-08-02    0.0\n",
       "1985-08-03    0.0\n",
       "1985-08-04    0.0\n",
       "1985-08-05    0.0\n",
       "             ... \n",
       "1985-12-27    0.0\n",
       "1985-12-28    0.0\n",
       "1985-12-29    0.0\n",
       "1985-12-30    5.0\n",
       "1985-12-31    2.0\n",
       "Name: SNOW, Length: 153, dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filling in Missing Values\n",
    "\n",
    "(\n",
    "    snow\n",
    "    # Select data from Aug - Dec 1985\n",
    "    .loc['1985-08':'1985-12']\n",
    "    # Fill the Null Values with 0\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The best way to do with missing data is the talk to a subject matter\n",
    "expert and determine why it is missing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-01    2.0\n",
       "1980-01-02    3.0\n",
       "1980-01-03    1.0\n",
       "1980-01-04    0.0\n",
       "1980-01-05    0.0\n",
       "             ... \n",
       "2019-09-03    0.0\n",
       "2019-09-04    0.0\n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: SNOW, Length: 13795, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Drop all missing values\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Only use this method after talking to a subject matter expert who confirms\n",
    "that it is ok to drop the data. It can be hard to tell later if the data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifting Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-01    NaN\n",
       "1980-01-02    2.0\n",
       "1980-01-03    3.0\n",
       "1980-01-04    1.0\n",
       "1980-01-05    0.0\n",
       "             ... \n",
       "2019-09-03    0.0\n",
       "2019-09-04    0.0\n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: SNOW, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shifting data forward\n",
    "\n",
    "snow.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-01    3.0\n",
       "1980-01-02    1.0\n",
       "1980-01-03    0.0\n",
       "1980-01-04    0.0\n",
       "1980-01-05    1.0\n",
       "             ... \n",
       "2019-09-03    0.0\n",
       "2019-09-04    0.0\n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    NaN\n",
       "Name: SNOW, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shifting data backward\n",
    "\n",
    "snow.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-01    NaN\n",
       "1980-01-02    NaN\n",
       "1980-01-03    NaN\n",
       "1980-01-04    NaN\n",
       "1980-01-05    1.2\n",
       "             ... \n",
       "2019-09-03    0.0\n",
       "2019-09-04    0.0\n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: SNOW, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Calculate the 5-Day Moving Average\n",
    "    .rolling(5)\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-31    20.0\n",
       "1980-02-29    25.0\n",
       "1980-03-31    16.0\n",
       "1980-04-30    10.0\n",
       "1980-05-31     9.0\n",
       "              ... \n",
       "2019-05-31     5.1\n",
       "2019-06-30     0.0\n",
       "2019-07-31     0.0\n",
       "2019-08-31     0.0\n",
       "2019-09-30     0.0\n",
       "Freq: M, Name: SNOW, Length: 477, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum rainfall by month\n",
    "\n",
    "(\n",
    "    snow\n",
    "    # Group by month\n",
    "    .resample('M')\n",
    "    # Get the max value for each month\n",
    "    .max()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The .resample method is used to aggregate values at different levels. At a high level, it groups date entries by some interval (yearly, monthly, weekly) and then aggregate the values at that interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-31    20.0\n",
       "1980-03-31    25.0\n",
       "1980-05-31    10.0\n",
       "1980-07-31     1.0\n",
       "1980-09-30     0.0\n",
       "              ... \n",
       "2019-01-31    19.0\n",
       "2019-03-31    20.7\n",
       "2019-05-31    18.0\n",
       "2019-07-31     0.0\n",
       "2019-09-30     0.0\n",
       "Freq: 2M, Name: SNOW, Length: 239, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Group by every 2 months\n",
    "    .resample('2M')\n",
    "    # Find the max for the grouped months\n",
    "    .max()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we want to aggregate the maximum value for each ski season, which normally ends in May,\n",
    "we could use the following code. This offset alias, 'A-MAY', indicates that we want an annual\n",
    "grouping ('A'), but ending in May of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-05-31    25.0\n",
       "1981-05-31    26.0\n",
       "1982-05-31    34.0\n",
       "1983-05-31    38.0\n",
       "1984-05-31    25.0\n",
       "1985-05-31    22.0\n",
       "1986-05-31    34.0\n",
       "1987-05-31    16.0\n",
       "1988-05-31    23.0\n",
       "1989-05-31    30.0\n",
       "1990-05-31    32.0\n",
       "1991-05-31    28.0\n",
       "1992-05-31    22.0\n",
       "1993-05-31    30.0\n",
       "1994-05-31    36.0\n",
       "1995-05-31    25.0\n",
       "1996-05-31    34.0\n",
       "1997-05-31    22.0\n",
       "1998-05-31    29.0\n",
       "1999-05-31    26.0\n",
       "2000-05-31    23.0\n",
       "2001-05-31    19.0\n",
       "2002-05-31    28.0\n",
       "2003-05-31    14.0\n",
       "2004-05-31    24.0\n",
       "2005-05-31    31.0\n",
       "2006-05-31    27.0\n",
       "2007-05-31    15.0\n",
       "2008-05-31    21.0\n",
       "2009-05-31    23.0\n",
       "2010-05-31    32.0\n",
       "2011-05-31    22.0\n",
       "2012-05-31    18.0\n",
       "2013-05-31    19.0\n",
       "2014-05-31    11.0\n",
       "2015-05-31    25.0\n",
       "2016-05-31    15.0\n",
       "2017-05-31    26.0\n",
       "2018-05-31    21.8\n",
       "2019-05-31    20.7\n",
       "2020-05-31     0.0\n",
       "Freq: A-MAY, Name: SNOW, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Group by Each Year ending in May\n",
    "    .resample('A-May')\n",
    "    # Find the max for each year\n",
    "    .max()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Aggregate Values (with Index)\n",
    "\n",
    "Below, instead of performing an aggregation with `.resample`, we leverage the\n",
    "method, `.transform` which works on aggregation groups but returns a series with the original index. This makes it easy to do things like calculate the percentage of quarterly snowfall the fell in a day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980-01-01    0.527009\n",
       "1980-01-02    0.790514\n",
       "1980-01-03    0.263505\n",
       "1980-01-04    0.000000\n",
       "1980-01-05    0.000000\n",
       "                ...   \n",
       "2019-09-03    0.000000\n",
       "2019-09-04    0.000000\n",
       "2019-09-05    0.000000\n",
       "2019-09-06    0.000000\n",
       "2019-09-07    0.000000\n",
       "Name: SNOW, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Divide by the total snowfall for that Quarter\n",
    "    .div( snow\n",
    "          # Aggregate by the Quarter  \n",
    "          .resample('Q')\n",
    "          # Transform and get the sum for the quarter and return it indexed\n",
    "          .transform('sum')\n",
    "         )\n",
    "    # Multiply by 100 to get the percentage\n",
    "    .mul(100)\n",
    "    # Fill null values with 0\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-01-31    23.631509\n",
       "2019-02-28    29.559413\n",
       "2019-03-31    24.939920\n",
       "2019-04-30    14.926569\n",
       "2019-05-31     6.942590\n",
       "2019-06-30     0.000000\n",
       "2019-07-31     0.000000\n",
       "2019-08-31     0.000000\n",
       "2019-09-30     0.000000\n",
       "Freq: M, Name: SNOW, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Select the year 2019\n",
    "    .loc['2019-01':'2019-12']\n",
    "    # Resample by monthly interval\n",
    "    .resample('M')\n",
    "    # Find the sum of each month\n",
    "    .sum()\n",
    "    # Divide by the total snowfall for that year\n",
    "    .div( \n",
    "         snow.loc['2019-01':'2019-12']\n",
    "        #  Total snow fall for the year\n",
    "        .sum()\n",
    "         )\n",
    "    # Multiply by 100\n",
    "    .mul(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby Operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that will determine ski season by looking at the index with date information. \n",
    "# It considers a season to be from October to September and shifts the remainder (Oct - Dec) to the next year\n",
    "\n",
    "def season(idx):\n",
    "    year = idx.year\n",
    "    month = idx.month\n",
    "    return year.where((month < 10), year + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980    457.5\n",
       "1981    503.0\n",
       "1982    842.5\n",
       "1983    807.5\n",
       "1984    816.0\n",
       "1985    536.0\n",
       "1986    740.8\n",
       "1987    243.1\n",
       "1988    314.5\n",
       "1989    429.5\n",
       "1990    331.5\n",
       "1991    504.7\n",
       "1992    340.8\n",
       "1993    683.5\n",
       "1994    321.0\n",
       "1995    645.0\n",
       "1996    525.5\n",
       "1997    563.6\n",
       "1998    579.6\n",
       "1999    435.7\n",
       "2000    453.0\n",
       "2001    468.0\n",
       "2002    457.8\n",
       "2003    365.4\n",
       "2004    514.0\n",
       "2005    472.0\n",
       "2006    594.6\n",
       "2007    319.7\n",
       "2008    606.0\n",
       "2009    476.8\n",
       "2010    391.0\n",
       "2011    533.8\n",
       "2012    293.5\n",
       "2013    362.8\n",
       "2014    358.7\n",
       "2015    284.3\n",
       "2016    354.6\n",
       "2017    524.0\n",
       "2018    308.8\n",
       "2019    504.5\n",
       "Name: SNOW, dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Groupby the season\n",
    "    .groupby(season)\n",
    "    # Aggregate by finding the sum for each season\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Operations\n",
    "\n",
    "There are also a handful of cumulative methods that work well with sequence data. These are\n",
    "`.cummin`, `.cummax`, `.cumprod`, and `.cumsum`. They return the cumulative minimum, maximum, product,\n",
    "and sum respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-10-01      0.0\n",
       "2016-10-02      0.0\n",
       "2016-10-03      4.9\n",
       "2016-10-04      4.9\n",
       "2016-10-05      5.5\n",
       "              ...  \n",
       "2017-10-27    530.9\n",
       "2017-10-28    530.9\n",
       "2017-10-29    530.9\n",
       "2017-10-30    530.9\n",
       "2017-10-31    530.9\n",
       "Name: SNOW, Length: 395, dtype: float64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Select Oct 2018 -Oct 2019 data\n",
    "    .loc['2016-10':'2017-10']\n",
    "    .cumsum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cumulative sum of a groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-10-01     0.0\n",
       "2016-10-02     0.0\n",
       "2016-10-03     4.9\n",
       "2016-10-04     4.9\n",
       "2016-10-05     5.5\n",
       "              ... \n",
       "2016-11-26    27.7\n",
       "2016-11-27    43.0\n",
       "2016-11-28     NaN\n",
       "2016-11-29    49.0\n",
       "2016-11-30    49.0\n",
       "Name: SNOW, Length: 61, dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    snow\n",
    "    # Select Oct 2016 -Nov 2016 data\n",
    "    .loc['2016-10':'2016-11']\n",
    "    # Group By the month\n",
    "    .resample('M')\n",
    "    # Unpack the records with transform and get the cumsum\n",
    "    .transform('cumsum')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, it we wanted to do this calculation for every year, we can combine `.resample` with\n",
    "`.transform` and `'cumsum'`:\n",
    "```\n",
    "    (snow\n",
    "    .resample('A-SEP ')\n",
    "    .transform('cumsum ')\n",
    "    )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "985dadb451a3879b46f97190c5b4bf917a8e90272eb49e4d6fad804dd6a2b5b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
